<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Quick Speech Test</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
        .section { margin: 20px 0; padding: 20px; border: 1px solid #ddd; border-radius: 8px; }
        button { background: #007acc; color: white; border: none; padding: 10px 20px; border-radius: 4px; cursor: pointer; margin: 5px; }
        .result { background: #f5f5f5; padding: 15px; border-radius: 4px; margin: 10px 0; white-space: pre-wrap; font-family: monospace; }
        .error { background: #ffe6e6; border-left: 4px solid #ff0000; }
        .success { background: #e6ffe6; border-left: 4px solid #00ff00; }
    </style>
</head>
<body>
    <h1>ğŸ”¬ Quick Speech Recognition Test</h1>
    
    <div class="section">
        <h2>ğŸ¯ Test 1: Azure Configuration Check</h2>
        <button onclick="testAzureConfig()">Test Azure Setup</button>
        <div id="configResult" class="result" style="display: none;"></div>
    </div>

    <div class="section">
        <h2>ğŸ¤ Test 2: Record Simple Audio</h2>
        <p>Record yourself saying: <strong>"Hello world"</strong></p>
        <button id="recordBtn" onclick="toggleRecording()">ğŸ”´ Start Recording</button>
        <button id="testRecordBtn" onclick="testRecording()" style="display: none;">ğŸ¯ Test Recording</button>
        <audio id="recordedAudio" controls style="display: none; width: 100%; margin: 10px 0;"></audio>
        <div id="recordResult" class="result" style="display: none;"></div>
    </div>

    <div class="section">
        <h2>ğŸ“ Test 3: Upload Audio File</h2>
        <input type="file" id="audioFile" accept=".mp3,.wav,.m4a" />
        <button onclick="testUploadedFile()">ğŸ¯ Test Uploaded File</button>
        <button onclick="convertAndTestFile()">ğŸ”„ Convert to Mono & Test</button>
        <div id="uploadResult" class="result" style="display: none;"></div>
    </div>

    <div class="section">
        <h2>ğŸ”§ Audio Format Info</h2>
        <p><strong>Current Issue:</strong> Azure Speech Services works best with <strong>mono audio</strong> (single channel).</p>
        <p><strong>Your audio might be stereo</strong> which can cause recognition failures.</p>
        <button onclick="checkAudioFormat()">ğŸ“Š Check Last Uploaded File Format</button>
        <div id="formatResult" class="result" style="display: none;"></div>
    </div>

    <div class="section">
        <h2>ğŸ©º Quick Diagnosis</h2>
        <p><strong>Are you getting "All recognition methods failed"?</strong> Run this quick check:</p>
        <button onclick="runQuickDiagnosis()">ğŸ” Run Quick Diagnosis</button>
        <div id="diagnosisResult" class="result" style="display: none;"></div>
    </div>

    <script>
        let mediaRecorder;
        let recordedBlob = null;
        let isRecording = false;

        async function testAzureConfig() {
            const resultDiv = document.getElementById('configResult');
            resultDiv.style.display = 'block';
            resultDiv.innerHTML = 'ğŸ”„ Testing Azure configuration...\n';

            try {
                // Test with minimal empty request to check Azure keys
                const response = await fetch('/api/speech-to-text', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({}) // Empty request to trigger validation
                });

                const result = await response.json();
                
                if (result.error === 'Azure Speech Service configuration missing') {
                    resultDiv.className = 'result error';
                    resultDiv.innerHTML = `âŒ AZURE CONFIGURATION PROBLEM

Error: ${result.error}
Details: ${result.details}

ğŸ”§ SOLUTION NEEDED:
1. Go to Azure Portal
2. Navigate to your Static Web App
3. Go to Configuration > Application Settings
4. Add these environment variables:
   - AZURE_SPEECH_KEY: (your speech service key)
   - AZURE_SPEECH_REGION: (your speech service region)

Debug Info: ${JSON.stringify(result.debug, null, 2)}`;
                } else if (result.error === 'Missing audio data') {
                    resultDiv.className = 'result success';
                    resultDiv.innerHTML = `âœ… AZURE CONFIGURATION IS WORKING!

The API responded correctly to the test request.
Azure Speech Service keys are properly configured.

Next step: Test with actual audio data.`;
                } else {
                    resultDiv.className = 'result error';
                    resultDiv.innerHTML = `âš ï¸ UNEXPECTED RESPONSE:

${JSON.stringify(result, null, 2)}`;
                }

            } catch (error) {
                resultDiv.className = 'result error';
                resultDiv.innerHTML = `âŒ API CONNECTION FAILED

Error: ${error.message}

ğŸ”§ POSSIBLE CAUSES:
1. Azure Function is not deployed
2. Network connectivity issues
3. API endpoint is incorrect
4. Azure Function is not running`;
            }
        }

        async function toggleRecording() {
            if (!isRecording) {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    mediaRecorder = new MediaRecorder(stream);
                    const chunks = [];

                    mediaRecorder.ondataavailable = (event) => chunks.push(event.data);
                    mediaRecorder.onstop = () => {
                        recordedBlob = new Blob(chunks, { type: 'audio/wav' });
                        const audioURL = URL.createObjectURL(recordedBlob);
                        
                        const audioElement = document.getElementById('recordedAudio');
                        audioElement.src = audioURL;
                        audioElement.style.display = 'block';
                        
                        document.getElementById('testRecordBtn').style.display = 'inline-block';
                        
                        const resultDiv = document.getElementById('recordResult');
                        resultDiv.style.display = 'block';
                        resultDiv.className = 'result';
                        resultDiv.innerHTML = `âœ… Recording complete! (${(recordedBlob.size / 1024).toFixed(1)} KB)\n\nClick "Test Recording" to test speech recognition.`;
                    };

                    mediaRecorder.start();
                    isRecording = true;
                    document.getElementById('recordBtn').innerHTML = 'â¹ï¸ Stop Recording (say "Hello world")';
                    
                } catch (error) {
                    alert('Microphone access denied: ' + error.message);
                }
            } else {
                mediaRecorder.stop();
                mediaRecorder.stream.getTracks().forEach(track => track.stop());
                isRecording = false;
                document.getElementById('recordBtn').innerHTML = 'ğŸ”´ Start Recording';
            }
        }

        async function testRecording() {
            if (!recordedBlob) {
                alert('No recording available');
                return;
            }

            await testAudioBlob(recordedBlob, 'recordResult', 'Recorded Audio');
        }

        async function testUploadedFile() {
            const fileInput = document.getElementById('audioFile');
            const file = fileInput.files[0];
            
            if (!file) {
                alert('Please select a file first');
                return;
            }

            await testAudioBlob(file, 'uploadResult', 'Uploaded File (Original)');
        }

        async function convertAndTestFile() {
            const fileInput = document.getElementById('audioFile');
            const file = fileInput.files[0];
            
            if (!file) {
                alert('Please select a file first');
                return;
            }

            const resultDiv = document.getElementById('uploadResult');
            resultDiv.style.display = 'block';
            resultDiv.className = 'result';
            resultDiv.innerHTML = 'ğŸ”„ Converting audio to mono format...\n';

            try {
                const monoBlob = await convertToMono(file);
                resultDiv.innerHTML += `âœ… Converted to mono! Original: ${(file.size/1024).toFixed(1)}KB â†’ Mono: ${(monoBlob.size/1024).toFixed(1)}KB\n\n`;
                await testAudioBlob(monoBlob, 'uploadResult', 'Uploaded File (Converted to Mono)');
            } catch (error) {
                resultDiv.className = 'result error';
                resultDiv.innerHTML += `âŒ Conversion failed: ${error.message}\n\nTrying original file instead...\n\n`;
                await testAudioBlob(file, 'uploadResult', 'Uploaded File (Original - Conversion Failed)');
            }
        }

        async function checkAudioFormat() {
            const fileInput = document.getElementById('audioFile');
            const file = fileInput.files[0];
            
            if (!file) {
                alert('Please select a file first');
                return;
            }

            const resultDiv = document.getElementById('formatResult');
            resultDiv.style.display = 'block';
            resultDiv.className = 'result';
            resultDiv.innerHTML = 'ğŸ” Analyzing audio format...\n';

            try {
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const arrayBuffer = await file.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

                const channels = audioBuffer.numberOfChannels;
                const sampleRate = audioBuffer.sampleRate;
                const duration = audioBuffer.duration;

                resultDiv.innerHTML = `ğŸ“Š Audio Format Analysis:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ File: ${file.name}
ğŸ“ Size: ${(file.size / 1024 / 1024).toFixed(2)} MB
ğŸµ Format: ${file.type || 'Unknown'}
â±ï¸ Duration: ${duration.toFixed(2)} seconds

ğŸ”Š Audio Properties:
   â€¢ Channels: ${channels} ${channels === 1 ? '(MONO âœ…)' : channels === 2 ? '(STEREO âš ï¸)' : '(MULTI-CHANNEL âŒ)'}
   â€¢ Sample Rate: ${sampleRate} Hz ${sampleRate === 16000 ? '(OPTIMAL âœ…)' : sampleRate === 44100 ? '(GOOD âœ“)' : '(âš ï¸)'}
   â€¢ Bit Depth: ~16-bit (estimated)

ğŸ’¡ Recommendations:
${channels === 1 ? 'âœ… Mono audio - should work well with Azure Speech' : 'âš ï¸ Convert to MONO for better Azure Speech recognition'}
${sampleRate === 16000 ? 'âœ… Optimal sample rate for speech recognition' : 'ğŸ’¡ 16kHz sample rate is optimal for speech recognition'}

ğŸ¯ Action: ${channels === 1 && sampleRate === 16000 ? 'Your audio is already optimized!' : 'Try the "Convert to Mono & Test" button above'}`;

            } catch (error) {
                resultDiv.className = 'result error';
                resultDiv.innerHTML = `âŒ Could not analyze audio format: ${error.message}

This might indicate:
â€¢ Unsupported audio format
â€¢ Corrupted audio file
â€¢ Browser compatibility issue

Try using a WAV file for best results.`;
            }
        }

        async function convertToMono(file) {
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
            
            // Read and decode the audio file
            const arrayBuffer = await file.arrayBuffer();
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
            
            // If already mono, return as-is
            if (audioBuffer.numberOfChannels === 1) {
                return file;
            }
            
            // Convert to mono by mixing channels
            const length = audioBuffer.length;
            const sampleRate = 16000; // Optimal for Azure Speech
            const numberOfChannels = 1; // Mono
            
            // Create offline context for processing
            const offlineContext = new OfflineAudioContext(numberOfChannels, Math.floor(length * sampleRate / audioBuffer.sampleRate), sampleRate);
            
            // Create buffer source
            const source = offlineContext.createBufferSource();
            source.buffer = audioBuffer;
            
            // Create channel merger to mix stereo to mono
            if (audioBuffer.numberOfChannels > 1) {
                const merger = offlineContext.createChannelMerger(1);
                const splitter = offlineContext.createChannelSplitter(audioBuffer.numberOfChannels);
                
                source.connect(splitter);
                
                // Mix all channels to mono
                for (let i = 0; i < audioBuffer.numberOfChannels; i++) {
                    const gainNode = offlineContext.createGain();
                    gainNode.gain.value = 1 / audioBuffer.numberOfChannels; // Equal mix
                    splitter.connect(gainNode, i);
                    gainNode.connect(merger, 0, 0);
                }
                
                merger.connect(offlineContext.destination);
            } else {
                source.connect(offlineContext.destination);
            }
            
            source.start();
            
            // Render the audio
            const renderedBuffer = await offlineContext.startRendering();
            
            // Convert to WAV blob
            return audioBufferToWav(renderedBuffer);
        }

        // Convert AudioBuffer to WAV blob (mono, 16kHz, 16-bit)
        function audioBufferToWav(buffer) {
            const length = buffer.length;
            const arrayBuffer = new ArrayBuffer(44 + length * 2);
            const view = new DataView(arrayBuffer);

            // WAV header
            const writeString = (offset, string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };

            writeString(0, 'RIFF');
            view.setUint32(4, 36 + length * 2, true);
            writeString(8, 'WAVE');
            writeString(12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true); // PCM
            view.setUint16(22, 1, true); // Mono
            view.setUint32(24, buffer.sampleRate, true);
            view.setUint32(28, buffer.sampleRate * 2, true);
            view.setUint16(32, 2, true);
            view.setUint16(34, 16, true);
            writeString(36, 'data');
            view.setUint32(40, length * 2, true);

            // Convert float samples to 16-bit PCM
            const channelData = buffer.getChannelData(0);
            let offset = 44;
            for (let i = 0; i < length; i++) {
                const sample = Math.max(-1, Math.min(1, channelData[i]));
                view.setInt16(offset, sample * 0x7FFF, true);
                offset += 2;
            }

            return new Blob([arrayBuffer], { type: 'audio/wav' });
        }

        async function testAudioBlob(blob, resultDivId, testName) {
            const resultDiv = document.getElementById(resultDivId);
            resultDiv.style.display = 'block';
            resultDiv.className = 'result';
            resultDiv.innerHTML = `ğŸ”„ Testing ${testName}...\n`;

            try {
                // Convert to base64
                const base64 = await blobToBase64(blob);
                
                resultDiv.innerHTML += `ğŸ“ Audio converted to base64 (${base64.length} chars)\n`;
                resultDiv.innerHTML += `ğŸŒ Sending to Azure Speech Service...\n`;

                const response = await fetch('/api/speech-to-text', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        audioData: base64,
                        format: blob.type || 'audio/wav'
                    })
                });

                const result = await response.json();

                if (result.success) {
                    resultDiv.className = 'result success';
                    resultDiv.innerHTML += `\nâœ… SUCCESS! Speech recognized:\n\n`;
                    resultDiv.innerHTML += `ğŸ—£ï¸ Text: "${result.text}"\n`;
                    resultDiv.innerHTML += `ğŸ“Š Debug: ${JSON.stringify(result.debug, null, 2)}`;
                } else {
                    resultDiv.className = 'result error';
                    resultDiv.innerHTML += `\nâŒ FAILED: ${result.error}\n\n`;
                    resultDiv.innerHTML += `ğŸ“ Details: ${result.details}\n\n`;
                    
                    if (result.debug) {
                        resultDiv.innerHTML += `ğŸ” Debug Information:\n`;
                        resultDiv.innerHTML += JSON.stringify(result.debug, null, 2) + '\n\n';
                    }
                    
                    if (result.troubleshooting) {
                        resultDiv.innerHTML += `ğŸ’¡ Suggestions:\n`;
                        result.troubleshooting.suggestions?.forEach((suggestion, i) => {
                            resultDiv.innerHTML += `   ${i + 1}. ${suggestion}\n`;
                        });
                    }
                }

            } catch (error) {
                resultDiv.className = 'result error';
                resultDiv.innerHTML += `\nâŒ ERROR: ${error.message}`;
            }
        }

        function blobToBase64(blob) {
            return new Promise((resolve, reject) => {
                const reader = new FileReader();
                reader.onload = () => {
                    const base64 = reader.result.split(',')[1];
                    resolve(base64);
                };
                reader.onerror = reject;
                reader.readAsDataURL(blob);
            });
        }

        async function runQuickDiagnosis() {
            const resultDiv = document.getElementById('diagnosisResult');
            resultDiv.style.display = 'block';
            resultDiv.className = 'result';
            resultDiv.innerHTML = 'ğŸ” Running quick diagnosis...\n\n';

            // Step 1: Test Azure config
            resultDiv.innerHTML += '1ï¸âƒ£ Testing Azure Configuration...\n';
            try {
                const response = await fetch('/api/speech-to-text', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({}) // Empty request
                });

                const result = await response.json();
                
                if (result.error === 'Missing audio data') {
                    resultDiv.innerHTML += '   âœ… Azure configuration is working\n\n';
                } else if (result.error === 'Azure Speech Service configuration missing') {
                    resultDiv.className = 'result error';
                    resultDiv.innerHTML += '   âŒ Azure configuration FAILED\n';
                    resultDiv.innerHTML += '   ğŸ”§ SOLUTION: Set up Azure environment variables\n\n';
                    resultDiv.innerHTML += '   Go to Azure Portal â†’ Static Web App â†’ Configuration\n';
                    resultDiv.innerHTML += '   Add: AZURE_SPEECH_KEY and AZURE_SPEECH_REGION\n\n';
                    return;
                } else {
                    resultDiv.innerHTML += `   âš ï¸ Unexpected response: ${result.error}\n\n`;
                }
            } catch (error) {
                resultDiv.className = 'result error';
                resultDiv.innerHTML += `   âŒ API connection failed: ${error.message}\n\n`;
                return;
            }

            // Step 2: Check if audio file is selected
            const fileInput = document.getElementById('audioFile');
            const file = fileInput.files[0];
            
            if (!file) {
                resultDiv.innerHTML += '2ï¸âƒ£ No audio file selected\n';
                resultDiv.innerHTML += '   ğŸ“ Please upload an audio file in Test 3 above, then run this again\n\n';
                return;
            }

            resultDiv.innerHTML += `2ï¸âƒ£ Audio file selected: ${file.name} (${(file.size/1024).toFixed(1)}KB)\n\n`;

            // Step 3: Check audio format
            resultDiv.innerHTML += '3ï¸âƒ£ Analyzing audio format...\n';
            try {
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const arrayBuffer = await file.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

                const channels = audioBuffer.numberOfChannels;
                const sampleRate = audioBuffer.sampleRate;
                
                resultDiv.innerHTML += `   Channels: ${channels} ${channels === 1 ? '(MONO âœ…)' : '(STEREO âš ï¸)'}\n`;
                resultDiv.innerHTML += `   Sample Rate: ${sampleRate}Hz\n\n`;

                if (channels > 1) {
                    resultDiv.innerHTML += 'ğŸ¯ LIKELY PROBLEM FOUND:\n';
                    resultDiv.innerHTML += '   Your audio is STEREO (2 channels)\n';
                    resultDiv.innerHTML += '   Azure Speech works best with MONO audio\n\n';
                    resultDiv.innerHTML += 'ğŸ’¡ SOLUTION:\n';
                    resultDiv.innerHTML += '   Click "ğŸ”„ Convert to Mono & Test" button above\n\n';
                } else {
                    resultDiv.innerHTML += '4ï¸âƒ£ Testing mono audio with Azure...\n';
                    
                    // Test the mono audio
                    const base64 = await blobToBase64(file);
                    const response = await fetch('/api/speech-to-text', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({
                            audioData: base64,
                            format: file.type || 'audio/wav'
                        })
                    });

                    const result = await response.json();
                    
                    if (result.success) {
                        resultDiv.className = 'result success';
                        resultDiv.innerHTML += `   âœ… SUCCESS! Recognized: "${result.text}"\n\n`;
                        resultDiv.innerHTML += 'ğŸ‰ Your audio works! The issue was probably temporary.\n';
                    } else {
                        resultDiv.className = 'result error';
                        resultDiv.innerHTML += `   âŒ Still failed: ${result.error}\n\n`;
                        resultDiv.innerHTML += 'ğŸ”§ OTHER POSSIBLE ISSUES:\n';
                        resultDiv.innerHTML += '   â€¢ Audio too quiet or contains no clear speech\n';
                        resultDiv.innerHTML += '   â€¢ Audio not in English\n';
                        resultDiv.innerHTML += '   â€¢ Audio file corrupted\n';
                        resultDiv.innerHTML += '   â€¢ Try recording a new short test audio\n';
                    }
                }

            } catch (error) {
                resultDiv.innerHTML += `   âŒ Could not analyze audio: ${error.message}\n\n`;
                resultDiv.innerHTML += '   Try using a WAV file for best results\n';
            }
        }
    </script>
</body>
</html>
